{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music generation with RNNs.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPXoLCck0RbBOf0OV0Jcoyy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asosawelford/MIT-Deep-Learning-6.S191/blob/main/Music_generation_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Music Generation with RNNs\n",
        "This notebook aims to generate a Recurring Neural Network (RNN) for music generation. We will make us of music written in [ABC notation](https://en.wikipedia.org/wiki/ABC_notation) to train a model that generates new music. This notebook as a part of a laboratory excercise, during [MIT 6.S191 Introduction to Deep Learning](https://introtodeeplearning.com). Visit their website for more information on ther great course!"
      ],
      "metadata": {
        "id": "cRCJZsQugGfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependancies and dataset\n",
        "Make sure to install the dependancies and ensure you are using a GPU (Runtime/Change Runtime Type/GPU)\n"
      ],
      "metadata": {
        "id": "TkZsqvBrieig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tensorflow 2.0\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf \n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtimes\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0"
      ],
      "metadata": {
        "id": "socQ7IhWixGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need a dataset, luckily MIT's library contains a dataset of irish songs in ABC notation, we can also listen to an example, by converting the MIDI data into audio."
      ],
      "metadata": {
        "id": "RDv3Bs54kodK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "# Print one of the songs to inspect it in greater detail!\n",
        "example_song = songs[0] #change this number to select some other song\n",
        "print(\"\\nExample song: \")\n",
        "print(example_song)"
      ],
      "metadata": {
        "id": "EG0atXtIk27o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the ABC notation to audio file and listen to it\n",
        "mdl.lab1.play_song(example_song)"
      ],
      "metadata": {
        "id": "d2bd3LDIk98O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the note-data (MIDI) contained in each song, we can also see there's metadata, such as song titles, key and tempo. The number of different charachters present in our training data will have an impact on the complexity of the learning problem. For now, we can join  our list of song strings into a single string containing all songs and find all unique characters in the joined string."
      ],
      "metadata": {
        "id": "qWCsnu8QlFtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join our list of song strings into a single string containing all songs\n",
        "songs_joined = \"\\n\\n\".join(songs) \n",
        "\n",
        "# Find all unique characters in the joined string\n",
        "vocab = sorted(set(songs_joined))\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
      ],
      "metadata": {
        "id": "sVcjJM9Jlkou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Processing\n",
        "Let's recap our learning task. An RNN, given a \"seed\" (that is, a collection of characters in secuential order) will predict what the next \"most probable\" characher will be. To achieve this, we will input a sequence of characters to the model, and train the model to predict the output. \n",
        "\n",
        "First off, we will vectorize the text (create a numerical representation of our text-based dataset).\n",
        "Let's create 2 lookup tables, one that maps characters to numbers, and a second that maps numbers back to characters."
      ],
      "metadata": {
        "id": "yfIPqUQDlsDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Define numerical representation of text ###\n",
        "\n",
        "# Create a mapping from character to unique index.\n",
        "# For example, to get the index of the character \"d\", \n",
        "#   we can evaluate `char2idx[\"d\"]`.  \n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "# Create a mapping from indices to characters. This is\n",
        "#   the inverse of char2idx and allows us to convert back\n",
        "#   from unique index to the character in our vocabulary.\n",
        "idx2char = np.array(vocab)"
      ],
      "metadata": {
        "id": "uMIr_zXvNWWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Vectorize the songs string ###\n",
        "\n",
        "'''TODO: Write a function to convert the all songs string to a vectorized\n",
        "    (i.e., numeric) representation. Use the appropriate mapping\n",
        "    above to convert from vocab characters to the corresponding indices.\n",
        "\n",
        "  NOTE: the output of the `vectorize_string` function \n",
        "  should be a np.array with `N` elements, where `N` is\n",
        "  the number of characters in the input string\n",
        "'''\n",
        "def vectorize_string(string):\n",
        "  vectorized_output = np.array([char2idx[char] for char in string])\n",
        "  return vectorized_output\n",
        "\n",
        "# def vectorize_string(string):\n",
        "  # TODO\n",
        "\n",
        "vectorized_songs = vectorize_string(songs_joined)"
      ],
      "metadata": {
        "id": "rLw0v5XsNXJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ],
      "metadata": {
        "id": "2_sZBwdCNUN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:9]), vectorized_songs[:9]))\n",
        "# check that vectorized_songs is a numpy array\n",
        "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
      ],
      "metadata": {
        "id": "A_zPOvgvNjMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training examples and targets\n",
        "Our next step is to actually divide the text into example sequences that we'll use during training. Each input sequence that we feed into our RNN will contain seq_length characters from the text. We'll also need to define a target sequence for each input sequence, which will be used in training the RNN to predict the next character. For each input, the corresponding target will contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "To do this, we'll break the text into chunks of seq_length+1. Suppose seq_length is 4 and our text is \"Hello\". Then, our input sequence is \"Hell\" and the target sequence is \"ello\".\n",
        "\n",
        "The batch method will then let us convert this stream of character indices to sequences of the desired size."
      ],
      "metadata": {
        "id": "WPdW-fyCOHoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Batch definition to create training examples ###\n",
        "\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "  # the length of the vectorized songs string\n",
        "  n = vectorized_songs.shape[0] - 1\n",
        "  # randomly choose the starting indices for the examples in the training batch\n",
        "  idx = np.random.choice(n-seq_length, batch_size)\n",
        "\n",
        "  '''TODO: construct a list of input sequences for the training batch'''\n",
        "  input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
        "  # input_batch = # TODO\n",
        "  '''TODO: construct a list of output sequences for the training batch'''\n",
        "  output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n",
        "  # output_batch = # TODO\n",
        "\n",
        "  # x_batch, y_batch provide the true inputs and targets for network training\n",
        "  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
        "  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
        "  return x_batch, y_batch\n"
      ],
      "metadata": {
        "id": "mKgEhhAuONQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33OHL3b84i0"
      },
      "source": [
        "For each of these vectors, each index is processed at a single time step. So, for the input at time step 0, the model receives the index for the first character in the sequence, and tries to predict the index of the next character. At the next timestep, it does the same thing, but the RNN considers the information from the previous step, i.e., its updated state, in addition to the current input.\n",
        "\n",
        "We can make this concrete by taking a look at how this works over the first several characters in our text:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
        "    print(\"Step {:3d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "metadata": {
        "id": "HlaOnTNwOXYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}